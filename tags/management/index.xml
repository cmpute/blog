<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Management on JacobZ</title><link>https://zyxin.xyz/blog/tags/management/</link><description>Recent content in Management on JacobZ</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sat, 10 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://zyxin.xyz/blog/tags/management/index.xml" rel="self" type="application/rss+xml"/><item><title>ACGN收藏 - 文件管理</title><link>https://zyxin.xyz/blog/2021-07/acgn-file-management/</link><pubDate>Sat, 10 Jul 2021 00:00:00 +0000</pubDate><guid>https://zyxin.xyz/blog/2021-07/acgn-file-management/</guid><description>&lt;p>对于ACGN收藏来说，文件管理是一个基础任务，毕竟收藏的文件内容多种多样，例如光盘镜像、压制后的音频视频、小册子扫描、字幕甚至小游戏等。把文件按一定结构整理是必要的，我也专门为整理音乐写了&lt;a class="link" href="https://github.com/cmpute/Fluss" target="_blank" rel="noopener"
>一些小工具&lt;/a>，不过整理文件的格式因人而异，也没有特别的难度，因此不需要特别描述我是怎么做的。我觉得值得一提的内容是如何对文件进行定期存档和备份，这也是我在硬盘被偷之后立马开始对收藏的文件进行的操作。备份有一个3-2-1的原则：3份备份，2份本地，1份云端，下面会介绍一些本地的备份和云端备份的方法以及我的选择。&lt;/p>
&lt;h2 id="离线备份">离线备份&lt;/h2>
&lt;p>离线备份就是把文件资料整理并存储到另一个设备上，需要考虑的功能有加密、压缩、增量更新、去重、冗余等。如果是最基本的备份，如果只想直接备份，不考虑加密压缩等的话，著名的&lt;a class="link" href="https://rsync.samba.org/" target="_blank" rel="noopener"
>rsync&lt;/a>是个不错的选择，它可以同步两个目录（可以是挂载FTP的目录），并且有算法来进行去重以减少二进制的传输。&lt;/p>
&lt;p>对我而言最大的需求是有冗余（指恢复记录，Recovery Record）和分卷，因为之前有在光盘上存一部分的音乐，而最后有几个压缩包已经无法恢复了，光这一条一卡几乎没剩下几条选项，可以参考&lt;a class="link" href="https://en.wikipedia.org/wiki/List_of_archive_formats#Data_recovery" target="_blank" rel="noopener"
>维基百科&lt;/a>。内置支持恢复记录的格式最著名且常用的有WinRAR（虽然不开源），另外剩下的里面开源的只有DAR，FreeArc。FreeArc已经10年没更新了，并且代码是毛子用Haskell写的，注释都是俄语。。因此就不考虑了。如果考虑外部支持的话最常用的就是Par2标准。下面对比几种（文件级别）方案的区别&lt;/p>
&lt;blockquote>
&lt;p>如果有多盘的话那么RAID就是不二选择了。不过选择文件系统以及组RAID或者NAS都是比较折腾，而且多数情况下需要Linux，我现在平时还是难免用Windows当主力，换成Linux做备份还是麻烦，因此本文就不介绍支持备份功能的文件系统了。如有兴趣可以自行了解&lt;a class="link" href="https://www.openzfs.org" target="_blank" rel="noopener"
>ZFS&lt;/a>、&lt;a class="link" href="https://btrfs.wiki.kernel.org/index.php/Main_Page" target="_blank" rel="noopener"
>Btrfs&lt;/a>或者&lt;a class="link" href="https://wiki.archlinux.org/title/XFS" target="_blank" rel="noopener"
>XFS&lt;/a>+&lt;a class="link" href="https://en.wikipedia.org/wiki/Logical_Volume_Manager_%28Linux%29" target="_blank" rel="noopener"
>LVM&lt;/a>。这方面还有有很多博文可以参考（如&lt;a class="link" href="https://markmcb.com/2020/01/07/five-years-of-btrfs" target="_blank" rel="noopener"
>这一篇ZFS和Btrfs的比较&lt;/a>，以及&lt;a class="link" href="https://ownyourbits.com/2019/03/03/how-to-recover-a-btrfs-partition/" target="_blank" rel="noopener"
>这一篇如何从Btrfs恢复数据&lt;/a>）&lt;/p>
&lt;/blockquote>
&lt;h3 id="winrar">WinRAR&lt;/h3>
&lt;p>&lt;a class="link" href="https://www.rarlab.com/" target="_blank" rel="noopener"
>WinRAR&lt;/a>除了不开源之外其实没有任何大毛病，它的解压部分也是开源的，因此不用担心以前的rar压缩包以后会打不开。主要的缺陷是WinRAR对增量更新几乎没有支持，最多&lt;a class="link" href="https://x443.wordpress.com/2012/07/11/winrar-incremental-differential-backup/" target="_blank" rel="noopener"
>通过文件flag来实现&lt;/a>，因此不必指望RAR做增量更新了。如果只是想把收藏做个镜像，那WinRAR就很方便了，有不错的压缩和加密，而且内置支持分卷和恢复记录，这两个功能到2021年仍然是独一家。&lt;/p>
&lt;h3 id="dar--par2">DAR + Par2&lt;/h3>
&lt;p>DAR是一个设计来替代Tar的文档格式，内置对Par2的支持，并且支持增量更新，对大量数据的备份其实挺友好的。&lt;a class="link" href="https://en.wikipedia.org/wiki/Parchive#Par2" target="_blank" rel="noopener"
>PAR2&lt;/a>是个给文件生成外部恢复记录的标准，可以生成一些恢复记录文件，当数据主体文件有一些损坏的时候，可以使用PAR2文件进行恢复，并且PAR2文件本身也是能够允许一部分损坏的。这个方案其实功能上来说很不错，但是由于是针对Linux设计的，对Windows支持用cygwin太不友好了。此外DAR的软件支持也不是很全，不知道为什么没有流行起来。&lt;/p>
&lt;h3 id="7zip--par2">7zip + Par2&lt;/h3>
&lt;p>如果不限打包软件（不要求对Par2的直接支持和增量更新）的话7zip应该是当前评价最高的压缩软件了。7zip + Par2是个不错的选择，不过设置Par2的参数就有一些麻烦了。这个方案相比WinRAR的优势仅仅在于7zip和Par2都是开源的。7zip有个额外的坏处是它的slice每个分区不能独立打开，rar的话每个slice包都有对应的文件可以解压。Par2相比WinRAR的修复好处在于它可以progressively提供冗余，就是下载的冗余文件不够的话可以下载更多冗余文件来进行修复，弱势是它不能处理32767以上个文件，因此必定需要跟某个archive格式一起使用。&lt;/p>
&lt;h3 id="7zip--seqbox">7zip + SeqBox&lt;/h3>
&lt;p>除了冗余数据之外，另一种保护对象是磁盘系统的文件头。SeqBox是一个用来保护&lt;strong>单一&lt;/strong>文件在磁盘文件系统损坏的情况下仍能恢复数据的通用工具，其工作原理是将文件分割成尺寸小于硬盘扇区（sector）大小的块，每个文件块有独立的包含文件UID的文件头，这样哪怕分区表损坏，指定文件还是可以通过一次全盘扫描恢复出来。而BlockyArchive则是基于此之上的改进版，给每个文件块加上了冗余码，使得文件本身的损坏也可以得到恢复。这个方法对数据长期冷存储应该是很有用的。不过它会产生不小的额外存储开销，并且对应的功能其实更适合通过文件系统本身来解决，例如之前提到的著名的ZFS和Btrfs。&lt;/p>
&lt;h2 id="在线备份">在线备份&lt;/h2>
&lt;p>由于在线存储服务商通常都会提供数据完整性check以及数据冗余存储的功能，因此对recovery record的需求没有那么大（百度网盘除外！！！辣鸡网盘下载经常文件损坏）。有许多软件支持数据同步和备份，同步比如Google Drive自带的sync，Onedrive或者&lt;a class="link" href="https://rclone.org/" target="_blank" rel="noopener"
>rclone&lt;/a>，他们的缺陷是没有加密、压缩，并且支持的snapshot功能有限。相比于本地备份，在线备份更关注的可能就是文件体积了，因为文件体积可能直接会影响收费策略，而冗余和备份通常会有云服务商来保证，因此去重对于在线备份来说是更重要的。&lt;/p>
&lt;p>更针对性的备份软件则对这些都有支持，在&lt;a class="link" href="https://alternativeto.net/software/time-machine/?license=opensource" target="_blank" rel="noopener"
>这个网站有一个开源软件的list&lt;/a>。这些软件通常支持将数据备份到另一个目录、NAS或者网盘，并且定期执行增量备份。由于Windows或者Mac目前还是不可避免地成为主力系统，因此只考虑支持Windows、Mac的情况下，再加上有GUI，可选项有&lt;a class="link" href="https://www.duplicati.com/" target="_blank" rel="noopener"
>Duplicati&lt;/a>，&lt;a class="link" href="https://duplicacy.com/" target="_blank" rel="noopener"
>Duplicacy&lt;/a>，&lt;a class="link" href="https://www.urbackup.org/impressions.html" target="_blank" rel="noopener"
>UrBackup&lt;/a>和&lt;a class="link" href="https://github.com/BlobBackup/BlobBackup" target="_blank" rel="noopener"
>BlobBackup&lt;/a>。这些软件有些是针对系统备份设计的，但其实我对系统备份没有什么需求，毕竟重装系统也没有很麻烦。Duplicacy有开源CLI，但GUI是收费的，性能很好。UrBackup的UI都很简陋，而且感觉更新不勤。BlockBackup是个定位简洁的产品，看下来Duplicati和Duplicacy还是个不错的选择，Duplicati支持的后端更多，而Duplicacy的性能更好并且更稳定。关于这些选择有不少比较，例如&lt;a class="link" href="https://forum.duplicati.com/t/big-comparison-borg-vs-restic-vs-arq-5-vs-duplicacy-vs-duplicati/9952" target="_blank" rel="noopener"
>Duplicati的论坛里&lt;/a>，&lt;a class="link" href="https://github.com/gilbertchen/benchmarking" target="_blank" rel="noopener"
>Duplicacy作者的benchmark&lt;/a>，可供参考。目前我的选择是Duplicacy，因为稳定并且高效。但Duplicacy由于算法特性，产生的文件块比较小，因此对于大数量的小文件备份不是很友好，如果之后要做日常文件备份的话可能还是会考虑Duplicati。&lt;/p>
&lt;p>这里提以下去重（Deduplication）和&lt;a class="link" href="http://dar.linux.free.fr/doc/usage_notes.html#Decremental_Backup" target="_blank" rel="noopener"
>增量（Incremental）/减量（Decremental）/差分（Differential）备份&lt;/a>的区别，通常增量备份仅仅会保留完整的新文件而可以跳过没有改动的文件（类似Git的模式），对文件中不同的部分一般不做处理，但在这种情况下如果有大文件进行了内容修改，则会产生大量的浪费，因此有专门的去重算法来针对文件整体内容进行去重，其本质上就是将所有文件看作一个大文件，然后通过特定的方法拆分（通常是使用&lt;a class="link" href="https://en.wikipedia.org/wiki/Rolling_hash" target="_blank" rel="noopener"
>Rolling hash&lt;/a>）来达到快速查重的效果。这样的一个比较大的问题就是文件会被分成很多小块（通常只有几个MB），因此对于文件传输来说其实很低效（例如上传到网盘、拷入备用磁盘等），并且将文件分块太细也会带来一定的性能和容量损失。在文件内容大部分为大文件，并且不会内部进行小修改的时候，这样的操作其实比较浪费时间。&lt;/p>
&lt;p>这里提到的在线备份工具都可以把本地磁盘看作一个备份目的地，因此也可以用作离线备份。另外离线备份也可以通过同步工具（如rclone）变成在线备份。上文提到的离线备份一般不能做到multi-version（除了ZFS），不过对于比如我这个音乐收集的任务来说，历史记录不是非常重要，因此也是个可行的方案。&lt;/p>
&lt;h2 id="网站归档">网站归档&lt;/h2>
&lt;p>还有一个比较另类的需求，我不仅想备份自己的文件，还想备份别人的&lt;del>文件&lt;/del>网站。&lt;/p>
&lt;p>很多同人社团的网站有很多信息，如Discography、世界观设定、Stuff List甚至一些正常的blog等，但是这些内容都不是持久的，很多同人社团停止活动之后网站也没了，因此也想备份他们的网站。这个需求通常可以通过知名网站&lt;a class="link" href="https://archive.org/web/" target="_blank" rel="noopener"
>Internet Archive&lt;/a>完成，但是这个网站因为是公益性质的，一些多媒体资源并不一定有保存下来，还是自己搭建网站爬虫会比较可靠，Internet Archive可以作为补充。&lt;/p>
&lt;p>网站爬取以前是通过IDM（Internet Download Manager）可以实现，但是IDM不免费因此后面也没有用了。而单页的存档方式之前很流行的一个格式是Firefox的&lt;a class="link" href="https://en.wikipedia.org/wiki/Mozilla_Archive_Format" target="_blank" rel="noopener"
>maff&lt;/a>，不过Firefox也不再支持这个格式了。现在的计划是下载单独的网页用Save Page WE这个插件来完成，基本可以原封不动地备份一个网页，而对于备份整个网站，计划之后搭建一个自己的&lt;a class="link" href="https://github.com/ArchiveBox/ArchiveBox" target="_blank" rel="noopener"
>ArchiveBox&lt;/a>。&lt;/p></description></item></channel></rss>